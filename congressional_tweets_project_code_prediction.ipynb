{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally was run on Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install kaggle\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install emoji\n",
    "!pip install accelerate\n",
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!touch ~/.kaggle/kaggle.json\n",
    "\n",
    "usr_name = ''\n",
    "key_ = ''\n",
    "\n",
    "api_token = {\"username\":usr_name,\"key\":key_}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c congressionaltweetcompetitionspring2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/congressionaltweetcompetitionspring2022.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change paths accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "train_valid_path = '/content/congressional_tweet_training_data.csv'\n",
    "test_path = '/content/congressional_tweet_test_data.csv'\n",
    "train_path = '/content/congressional_tweet_train_data.csv'\n",
    "valid_path = '/content/congressional_tweet_val_data.csv'\n",
    "subset_path = '/content/congressional_tweet_train_subset_data.csv'\n",
    "training_output_path = '/content/training_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_dev_split():\n",
    "  data = pd.read_csv(train_valid_path)\n",
    "  train, dev = train_test_split(data, test_size=0.05, random_state=456)\n",
    "\n",
    "  train.to_csv(train_path, index=False)\n",
    "  dev.to_csv(valid_path, index=False)\n",
    "\n",
    "def make_train_subset():\n",
    "  data = pd.read_csv(train_path)\n",
    "  _, subset = train_test_split(data, test_size=500, random_state=456)\n",
    "\n",
    "  subset.to_csv(subset_path, index=False)\n",
    "\n",
    "make_train_dev_split()\n",
    "make_train_subset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.party2id = {'D': 0, 'R': 1}\n",
    "        data = self._read_data(data_path)\n",
    "        self.input_strs, self.label_ids = self._convert_to_samples(data)\n",
    "        self.n_samples = len(self.input_strs)\n",
    "\n",
    "    def _read_data(self, path):\n",
    "        path = os.path.join(*path.split('\\\\'))\n",
    "        data = pd.read_csv(path)\n",
    "        data = data.drop_duplicates(['full_text'])\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _convert_to_samples(self, data):\n",
    "        input_strs = data['full_text'].tolist()\n",
    "        labels = data['party_id'].tolist()\n",
    "        input_strs_out, label_ids = [], []\n",
    "\n",
    "        for i_sample, (input_str, label) in enumerate(tqdm(zip(input_strs, labels))):\n",
    "            input_str = input_str.replace('\\\"b', '').replace('b\\'', '').replace('b\\\"', '').replace('\\'', '')\n",
    "            input_str = re.sub(r\"http\\S+\", \"\", input_str)\n",
    "            # tokenizer_out = self.tokenizer(input_str, truncation=True)\n",
    "            label_id = self.party2id[label]\n",
    "\n",
    "            input_strs_out.append(input_str)\n",
    "            label_ids.append(torch.tensor(label_id, dtype=torch.long))\n",
    "\n",
    "        return input_strs_out, label_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.input_strs[item], self.label_ids[item]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_strs, label_ids = zip(*batch)\n",
    "\n",
    "        tokenizer_out = self.tokenizer(list(input_strs), \n",
    "                                       truncation=True, \n",
    "                                       padding=True,\n",
    "                                       return_attention_mask=True,\n",
    "                                       return_tensors='pt')\n",
    "        input_ids = tokenizer_out['input_ids']\n",
    "        attention_mask = tokenizer_out['attention_mask']\n",
    "        label_ids = torch.tensor(label_ids)\n",
    "\n",
    "        return input_ids, attention_mask, label_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModel\n",
    "from transformers import logging as t_logging\n",
    "t_logging.set_verbosity_error()\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, pretrained_encoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = pretrained_encoder\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.frozen = True\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    def loss_fn(self, pred, target):\n",
    "        return self.loss(pred, target)\n",
    "    \n",
    "    def forward(self, batch, return_type='loss'):\n",
    "        input_ids, attention_mask, target_ids = batch        \n",
    "        \n",
    "        out, _ = self.encoder(input_ids = input_ids, \n",
    "                              attention_mask = attention_mask,\n",
    "                              return_dict=False)\n",
    "        out = torch.mean(out, dim=1)\n",
    "        logits = self.linear(out)\n",
    "        \n",
    "        if return_type == 'loss':\n",
    "            return self.loss_fn(logits, target_ids)\n",
    "        elif return_type == 'logits':\n",
    "            return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, accelerator, device, train_dataset, val_dataset=None, ckpt_path=None):\n",
    "        self.model = model\n",
    "        self.accelerator = accelerator\n",
    "        self.device = device\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset if val_dataset != None else train_dataset \n",
    "        \n",
    "        bsz_train = 32\n",
    "        bsz_val = 128\n",
    "        \n",
    "        self._set_seed()\n",
    "        self._init_model(ckpt_path)\n",
    "        self._get_dataloaders(bsz_train, bsz_val)\n",
    "        self._get_optimizer()\n",
    "    \n",
    "    def _set_seed(self):\n",
    "        self.seed = 69420\n",
    "        torch.manual_seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "    def _init_model(self, model_path):\n",
    "        if model_path:\n",
    "            path = os.path.join(*model_path.split('\\\\'))\n",
    "            self.model.load_state_dict(torch.load(path))\n",
    "        \n",
    "        # self.model = self.accelerator.prepare(self.model)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def _get_dataloaders(self, bsz_train, bsz_val):\n",
    "        train_loader = DataLoader(self.train_dataset,\n",
    "                                      batch_size=bsz_train,\n",
    "                                      collate_fn=self.train_dataset.collate_fn,\n",
    "                                      shuffle=True,\n",
    "                                      drop_last=True\n",
    "                                      )\n",
    "        val_loader = DataLoader(self.val_dataset,\n",
    "                                    batch_size=bsz_val,\n",
    "                                    collate_fn=self.val_dataset.collate_fn,\n",
    "                                    shuffle=False,\n",
    "                                    drop_last=False\n",
    "                                    )\n",
    "        self.train_loader, self.val_loader = train_loader, val_loader\n",
    "        # self.train_loader, self.val_loader = self.accelerator.prepare(train_loader, val_loader)\n",
    "    \n",
    "    def _get_optimizer(self):\n",
    "        model_params = list(self.model.named_parameters())\n",
    "        no_decay = ['bias']\n",
    "        optimized_params = [\n",
    "            {\n",
    "                'params':[p for n, p in model_params if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay': 0.01\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in model_params if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }   \n",
    "        ]\n",
    "        optimizer = AdamW(optimized_params, lr=0.001)\n",
    "        self.optimizer = optimizer\n",
    "        # self.optimizer = self.accelerator.prepare(optimizer)\n",
    "        \n",
    "    def run_train(self, n_epochs):\n",
    "        best_loss = self.run_validation()\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            pbar = tqdm(self.train_loader, disable = not self.accelerator.is_local_main_process)\n",
    "            batch_loss = 0\n",
    "            self.model.train()\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            \n",
    "            for i, batch in enumerate(pbar):\n",
    "                batch = tuple(item.to(self.device) for item in batch)\n",
    "                batch_loss = self._training_step(batch)\n",
    "                \n",
    "                self.optimizer.step()\n",
    "                self.model.zero_grad(set_to_none=True)\n",
    "                \n",
    "                pbar.set_description(f'(Training) Epoch: {epoch} Loss: {batch_loss:.4f}')\n",
    "            \n",
    "            val_loss = self.run_validation()\n",
    "            \n",
    "            if val_loss < best_loss:\n",
    "                print(f'New best validation loss at {val_loss:.4f}, saving checkpoint')\n",
    "                best_loss = val_loss\n",
    "                # self.accelerator.wait_for_everyone()\n",
    "                # unwrapped_model = self.accelerator.unwrap_model(self.model)\n",
    "                ckpt_path = os.path.join('model_ckpt.pt')\n",
    "                # torch.save(unwrapped_model.state_dict(), ckpt_path)\n",
    "                torch.save(self.model.state_dict(), ckpt_path)\n",
    "                print(f'New checkpoint saved at {ckpt_path}')\n",
    "            elif (val_loss >= best_loss or epoch > 2) and self.model.frozen == True:\n",
    "                print(f'Unfreeze encoder at epoch {epoch}')\n",
    "                self.model.frozen = False\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] = 0.00002\n",
    "                for param in self.model.encoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "    def run_validation(self):\n",
    "        pbar = tqdm(enumerate(self.val_loader), total=len(self.val_loader))\n",
    "        self.model.eval()\n",
    "        preds, labels = [], []\n",
    "        val_loss = 0\n",
    "        \n",
    "        for i, batch in pbar:\n",
    "            batch = tuple(item.to(self.device) for item in batch)\n",
    "            loss, pred = self._prediction_step(batch)\n",
    "            pbar.set_description(f'(Validating)')\n",
    "            val_loss += loss\n",
    "            \n",
    "            _, _, target_ids = batch\n",
    "            pred = [int(i) for i in pred.cpu().numpy()]\n",
    "            label = [int(i) for i in target_ids.cpu().numpy()]\n",
    "            \n",
    "            preds.extend(pred)\n",
    "            labels.extend(label)\n",
    "            \n",
    "        score = classification_report(labels, preds)\n",
    "        \n",
    "        print(score)\n",
    "        print(f' Validation loss: {val_loss:.4f}')\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def _training_step(self, batch):\n",
    "        loss = self.model(batch, return_type='loss')\n",
    "        loss.backward()\n",
    "        # self.accelerator.backward(loss)\n",
    "        \n",
    "        return loss.detach()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _prediction_step(self, batch):\n",
    "        loss = self.model(batch, return_type='loss')\n",
    "        preds = self.model(batch, return_type='logits')\n",
    "        preds = torch.argmax(preds, dim=1)\n",
    "        \n",
    "        return loss.detach(), preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "\n",
    "train_path = 'data\\congressional_tweet_train_data.csv'\n",
    "val_path = 'data\\congressional_tweet_val_data.csv'\n",
    "pretrained_model = 'vinai/bertweet-base'\n",
    "ckpt_path = None\n",
    "\n",
    "accelerator = Accelerator()\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "train_dataset = TweetDataset(train_path, tokenizer)\n",
    "val_dataset = TweetDataset(val_path, tokenizer)\n",
    "pretrained_encoder = AutoModel.from_pretrained(pretrained_model)\n",
    "model = Model(pretrained_encoder)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trainer = Trainer(model, accelerator, device, train_dataset, val_dataset, ckpt_path)\n",
    "trainer.run_train(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "def produce_test_prediction(dataset, dataloader, model, data_path):\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    \n",
    "    preds = []\n",
    "    for i, batch in pbar:\n",
    "        batch = tuple(item.to(device) for item in batch)\n",
    "        pred = model(batch, return_type='logits')\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        pbar.set_description(f'(Validating)')\n",
    "        \n",
    "        pred = [int(i) for i in pred.cpu().numpy()]        \n",
    "        preds.extend(pred)\n",
    "    \n",
    "    id2party = {v: k for k,v in dataset.party2id.items()}\n",
    "    preds = [id2party[pred] for pred in preds]\n",
    "    df_out = pd.DataFrame({'party': preds})\n",
    "    data_out = pd.read_csv(data_path)\n",
    "    data_out['pred'] = df_out['party']\n",
    "    \n",
    "    data_out.to_csv('pred_distilroberta_temp.csv', index=False)\n",
    "\n",
    "ckpt_path = 'distilroberta90.pt'\n",
    "pretrained_model = 'distilroberta-base'\n",
    "data_path = 'data\\congressional_tweet_training_data.csv'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "dataset = TweetDataset(data_path, tokenizer)\n",
    "pretrained_encoder = AutoModel.from_pretrained(pretrained_model)\n",
    "model = Model(pretrained_encoder)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=64,\n",
    "                        collate_fn=dataset.collate_fn,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False\n",
    "                        )\n",
    "\n",
    "produce_test_prediction(dataset, dataloader, model, data_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation          \n",
    "Parts of the code was taken from the author's previous works"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
